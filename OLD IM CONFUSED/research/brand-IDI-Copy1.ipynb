{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Word Frequency for Brand Perception Interviews\n",
    "amongst key stakeholders NYCAASC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-60474dd24e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# only need to run once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# only need to run once\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/chuamelia/Downloads/act_reviews.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at Structure of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Everything! It worked out smoothly</td>\n",
       "      <td>Asian American Media The speakers were interes...</td>\n",
       "      <td>Nothing, its great! More workshops?</td>\n",
       "      <td>More entertainment, like performers during sna...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    1  \\\n",
       "0  Everything! It worked out smoothly   \n",
       "\n",
       "                                                   2  \\\n",
       "0  Asian American Media The speakers were interes...   \n",
       "\n",
       "                                     3  \\\n",
       "0  Nothing, its great! More workshops?   \n",
       "\n",
       "                                                   4  \n",
       "0  More entertainment, like performers during sna...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Functions for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Word Types][Table of Word Type]\n",
    "[Table of Word Type]: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write function to append all tokens to one list.\n",
    "def stuff_tokenizer(column, list):\n",
    "    discard = ['IN', ',','.','TO', 'DT', 'PRP', 'CC', 'are', 'is', 'um', u'it\\u2019s', 'PRP$']\n",
    "    end_num = len(column)\n",
    "    temp = []\n",
    "    for i in range(end_num): #append to one list\n",
    "        temp.extend(word_tokenize(str(column[i]).decode('utf-8')))\n",
    "    temp2 = pos_tag(temp) #tag words\n",
    "    for i in temp2: #discard prepositions, articles, etc.\n",
    "        if i[1] not in discard and i[0] not in discard: \n",
    "            list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add decode('utf-8') bc \"\\xe2\\x80\\x99\" interprtation\n",
    "#ascii' codec can't decode byte\n",
    "#example: df['first_Time'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty lists for stuffing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1 = []\n",
    "q2 = []\n",
    "q3 = []\n",
    "q4 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuff Away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-332fe8393c09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstuff_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstuff_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstuff_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstuff_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-53e0456a0fae>\u001b[0m in \u001b[0;36mstuff_tokenizer\u001b[0;34m(column, list)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#append to one list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtemp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#tag words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#discard prepositions, articles, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "stuff_tokenizer(df['1'],q1)\n",
    "stuff_tokenizer(df['2'],q2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stuff_tokenizer(df['3'],q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stuff_tokenizer(df['4'],q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if stuffing worked..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Everything', 'NN'), (u'worked', 'VBD'), (u'out', 'RP')]\n"
     ]
    }
   ],
   "source": [
    "print q1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = [q1,q2,q3,q4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'people', 'NNS'), 9), ((u'workshops', 'NNS'), 9), ((u'liked', 'VBD'), 8), ((u'was', 'VBD'), 7), ((u'were', 'VBD'), 7)]\n",
      "[((u'was', 'VBD'), 9), ((u'-', ':'), 8), ((u'Media', 'NNP'), 7), ((u'very', 'RB'), 6), ((u':', ':'), 6)]\n",
      "[((u'More', 'JJR'), 11), ((u'workshops', 'NNS'), 7), ((u'time', 'NN'), 6), ((u'breakout', 'NN'), 5), ((u'more', 'RBR'), 4)]\n",
      "[((u'More', 'JJR'), 10), ((u'More', 'RBR'), 8), ((u'Asian', 'JJ'), 7), ((u'workshops', 'NNS'), 6), ((u'people', 'NNS'), 4)]\n"
     ]
    }
   ],
   "source": [
    "for i in dataset:\n",
    "    common = Counter(i)\n",
    "    print common.most_common(5)\n",
    "# Need to remove prepositions\n",
    "# How can we control for one person repeating the same word?\n",
    "# select distinct words: my_list = list(set(my_list))\n",
    "# compare word counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'More', 'JJR'), 10),\n",
       " ((u'More', 'RBR'), 8),\n",
       " ((u'Asian', 'JJ'), 7),\n",
       " ((u'workshops', 'NNS'), 6),\n",
       " ((u'people', 'NNS'), 4),\n",
       " ((u'panels', 'NNS'), 3),\n",
       " ((u'interactive', 'JJ'), 3),\n",
       " ((u'more', 'JJR'), 3),\n",
       " ((u'would', 'MD'), 3),\n",
       " ((u'see', 'VB'), 3),\n",
       " ((u'topics', 'NNS'), 3),\n",
       " ((u'AA', 'NNP'), 2),\n",
       " ((u'interaction', 'NN'), 2),\n",
       " ((u'performers', 'NNS'), 2),\n",
       " ((u'More', 'NNP'), 2),\n",
       " ((u'group', 'NN'), 2),\n",
       " ((u'Different', 'NNP'), 2),\n",
       " ((u'more', 'RBR'), 2),\n",
       " ((u'like', 'VB'), 2),\n",
       " ((u'culture', 'NN'), 2),\n",
       " ((u'sure', 'JJ'), 2),\n",
       " ((u'different', 'JJ'), 2),\n",
       " ((u'nan', 'RB'), 2),\n",
       " ((u'NYU', 'NNP'), 2),\n",
       " ((u'nan', 'JJ'), 2),\n",
       " ((u'time', 'NN'), 2),\n",
       " ((u'auditorium', 'NN'), 1),\n",
       " ((u'Form', 'NNP'), 1),\n",
       " ((u'acitivites', 'NNS'), 1),\n",
       " ((u'Jin', 'NNP'), 1),\n",
       " ((u'issues', 'NNS'), 1),\n",
       " ((u'media', 'NNS'), 1),\n",
       " ((u'dance', 'NN'), 1),\n",
       " ((u'track', 'NN'), 1),\n",
       " ((u'3rd', 'CD'), 1),\n",
       " ((u'form', 'VBP'), 1),\n",
       " ((u'Bubble', 'NNP'), 1),\n",
       " ((u'Water', 'NNP'), 1),\n",
       " ((u'broad', 'JJ'), 1),\n",
       " ((u'Hirano', 'NNP'), 1),\n",
       " ((u'effect', 'NN'), 1),\n",
       " ((u'come', 'VB'), 1),\n",
       " ((u'sucessful', 'JJ'), 1),\n",
       " ((u'``', '``'), 1),\n",
       " ((u'Women', 'NNP'), 1),\n",
       " ((u'same', 'JJ'), 1),\n",
       " ((u'economy', 'NN'), 1),\n",
       " ((u'Invite', 'NNP'), 1),\n",
       " ((u'fun', 'NN'), 1),\n",
       " ((u'who', 'WP'), 1),\n",
       " ((u'diversity', 'JJ'), 1),\n",
       " ((u'come', 'VBP'), 1),\n",
       " ((u'that', 'WDT'), 1),\n",
       " ((u'have', 'VB'), 1),\n",
       " ((u'great', 'JJ'), 1),\n",
       " ((u'enjoyd', 'VBP'), 1),\n",
       " ((u'snacktime', 'JJ'), 1),\n",
       " ((u'entertainment', 'NN'), 1),\n",
       " ((u'sit', 'VB'), 1),\n",
       " ((u'Same', 'NNP'), 1),\n",
       " ((u'Indian', 'JJ'), 1),\n",
       " ((u'particpants', 'NNS'), 1),\n",
       " ((u'schoolers', 'NNS'), 1),\n",
       " ((u'Tea', 'NNP'), 1),\n",
       " ((u'love', 'VB'), 1),\n",
       " ((u'interesting', 'JJ'), 1),\n",
       " ((u'high', 'JJ'), 1),\n",
       " ((u'Dancing', 'NNP'), 1),\n",
       " ((u'other', 'JJ'), 1),\n",
       " ((u'Show', 'VB'), 1),\n",
       " ((u'workshop', 'NN'), 1),\n",
       " ((u'Similar', 'JJ'), 1),\n",
       " ((u'schedule', 'VBZ'), 1),\n",
       " ((u'eat', 'VB'), 1),\n",
       " ((u'could', 'MD'), 1),\n",
       " ((u'catered', 'VBD'), 1),\n",
       " ((u'year', 'NN'), 1),\n",
       " ((u'came', 'VBD'), 1),\n",
       " ((u'workshps', 'VBD'), 1),\n",
       " ((u'dialogue', 'NN'), 1),\n",
       " ((u'application', 'NN'), 1),\n",
       " ((u'changing', 'VBG'), 1),\n",
       " ((u'think', 'VBP'), 1),\n",
       " ((u'crews', 'NNS'), 1),\n",
       " ((u'Call', 'NNP'), 1),\n",
       " ((u'change', 'VB'), 1),\n",
       " ((u'music', 'NN'), 1),\n",
       " ((u'tables', 'NNS'), 1),\n",
       " ((u'better', 'JJR'), 1),\n",
       " ((u'abuility', 'NN'), 1),\n",
       " ((u'becoming', 'VBG'), 1),\n",
       " ((u'anything', 'NN'), 1),\n",
       " ((u'audience', 'NN'), 1),\n",
       " ((u'should', 'MD'), 1),\n",
       " ((u'members', 'NNS'), 1),\n",
       " ((u'quality', 'NN'), 1),\n",
       " ((u'trafficing', 'VBG'), 1),\n",
       " ((u'has', 'VBZ'), 1),\n",
       " ((u'endure', 'VB'), 1),\n",
       " ((u'clubs', 'NNS'), 1),\n",
       " ((u'make', 'VB'), 1),\n",
       " ((u'campus', 'NN'), 1),\n",
       " ((u'cultural', 'JJ'), 1),\n",
       " ((u'identities', 'NNS'), 1),\n",
       " ((u'Society', 'NNP'), 1),\n",
       " ((u'struggles', 'NNS'), 1),\n",
       " ((u'experiences', 'NNS'), 1),\n",
       " ((u'activites', 'NNS'), 1),\n",
       " ((u'effort', 'NN'), 1),\n",
       " ((u'Empowerment', 'NN'), 1),\n",
       " ((u'large', 'JJ'), 1),\n",
       " ((u'roles', 'NNS'), 1),\n",
       " ((u')', ')'), 1),\n",
       " ((u'women', 'NNS'), 1),\n",
       " ((u'Bottled', 'NNP'), 1),\n",
       " ((u\"''\", \"''\"), 1),\n",
       " ((u'Americans', 'NNPS'), 1),\n",
       " ((u'Sex', 'NNP'), 1),\n",
       " ((u'(', '('), 1),\n",
       " ((u'specific', 'JJ'), 1),\n",
       " ((u'school', 'NN'), 1),\n",
       " ((u'groups', 'NNS'), 1),\n",
       " ((u'Feminity', 'NNP'), 1),\n",
       " ((u'Trafficking', 'NNP'), 1),\n",
       " ((u'enough', 'JJ'), 1),\n",
       " ((u'speakers', 'NNS'), 1),\n",
       " ((u'country', 'NN'), 1),\n",
       " ((u'Ex', 'NNP'), 1),\n",
       " ((u'things', 'NNS'), 1),\n",
       " ((u'Aya', 'NNP'), 1),\n",
       " ((u'general', 'JJ'), 1),\n",
       " ((u'bonding', 'JJ'), 1),\n",
       " ((u'art', 'NN'), 1),\n",
       " ((u'workshop', 'VBD'), 1),\n",
       " ((u'industry', 'NN'), 1),\n",
       " ((u'profession', 'NN'), 1),\n",
       " ((u'one', 'NN'), 1),\n",
       " ((u'Not', 'RB'), 1),\n",
       " ((u'Better', 'VBP'), 1),\n",
       " ((u'Cant', 'JJ'), 1),\n",
       " ((u'controversial', 'JJ'), 1),\n",
       " ((u'Maybe', 'RB'), 1),\n",
       " ((u'performances', 'NNS'), 1),\n",
       " ((u'Build', 'NNP'), 1),\n",
       " ((u'panelists', 'NNS'), 1),\n",
       " ((u'workshops', 'VBZ'), 1),\n",
       " ((u'hip/hop', 'NN'), 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(q4).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmn = Counter(q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmn = {k: v for k, v in cmn.iteritems() if  v > 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- Peter Norvig [Everything I Need to Know About NLP I learned from Sesame Street][a1]\n",
    "\n",
    "\n",
    "[a1]: http://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
